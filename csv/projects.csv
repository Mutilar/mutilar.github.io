ID,NAME,TITLE,DATE,LOCATION,GITHUB,MOTTO,WIN,TEXT
marp,ü§ñ MARP,Personal Project,Winter 2025 - Present,"Bellevue, Washington",https://github.com/Mutilar/Marp,"4-axis robot with wireless teleoperation &amp; real-time video streaming",,"<h3>The Idea</h3><p>MARP is a 4-axis robot platform with wireless teleoperation and real-time video streaming: a Raspberry Pi 5 brain, a Valve Steam Deck controller, and a modular architecture split across focused repos.</p><h3>How It Works</h3><p>The Pi 5 runs <a href='https://github.com/Mutilar/MarpPi' target='_blank'>marp.brain</a>: 4-axis stepper control (2 drive + 2 turret) via lgpio, dual input switching between USB joystick and Wi-Fi Direct UDP, MJPEG video multiplexing across Pi Camera and Kinect sources, and auto-stop safety on connection loss. The Steam Deck runs <a href='https://github.com/Mutilar/MarpGamepad' target='_blank'>marp.gamepad</a>: a Unity 6.0 teleoperation client with live video feed, native controller support, and source switching. Wi-Fi Direct lets the Pi act as its own hotspot for field operation without a router.</p><h3>The Grind</h3><p>Two repos, two platforms, one control loop. The Pi side is Python/C++ with systemd services for boot-ready deployment; the Deck side is C#/ShaderLab with automated build-and-deploy scripts. Kinect integration vendors libfreenect as a submodule for RGB, depth, IR, accelerometer, and motor/LED control.</p>"
azuremlops,‚ö° AzureMLOps,"MSFT Internship",Summer 2019,"Bellevue, Washington",https://github.com/Mutilar/AzureMLOperationalization,"Streamlining &amp; expediting CI/CD workflows",,"<h3>The Idea</h3><p>Azure ML Operationalization replaces agent-bound CI/CD with an agentless pipeline: Azure DevOps triggers an Azure Service Bus message, which fires an Azure Function that submits notebook Runs directly to Azure ML Compute. Notebooks validate without queuing behind other builds, and failures surface as DevOps Test Runs for debugging in the pipeline UI.</p><figure class='modal-figure'><img src='images/azuremlops-mission.png' alt='AzureML Ops mission and goals'></figure><h3>How It Works</h3><p>Three moving parts: an agentless DevOps Pipeline publishes a message to Azure Service Bus via <code>PublishToAzureServiceBus@1</code>, an Azure Function (<code>run_notebook_service_bus</code>) picks it up and orchestrates the run, and Azure ML Compute executes the notebook. The Function authenticates to the ML Workspace via Service Principal, fetches the target repository, injects try-catch callbacks around each notebook cell for completion signaling, submits Runs with configurable compute targets and Conda environments, and polls for results. When runs finish, the Function posts Test Run results, attachments, and status updates back to DevOps via the REST API, then closes the pipeline with a callback. A separate agent-based deployment pipeline handles <code>pytest</code> unit testing, packaging, and <code>AzureFunctionApp@1</code> deployment.</p><figure class='modal-figure'><img src='images/azuremlops-sequence.png' alt='Sequence diagram showing the agentless pipeline flow'></figure><h3>The Grind</h3><p>The codebase is split into focused handler modules: <code>azureml_handler.py</code> (Workspace auth, Experiment/Run/RunConfig management), <code>devops_handler.py</code> (pipeline callbacks, Test Run CRUD, attachments, repository downloads), <code>file_handler.py</code> (staging, snapshot builds, Conda dependency injection, notebook callback injection/removal), and <code>notebook_handler.py</code> (cell-level code injection and scrubbing). Pipeline configuration is driven by DevOps variables; 20+ parameters covering Service Bus connections, compute targets, Docker images, repository versions, and Service Principal credentials.</p><figure class='modal-figure'><img src='images/azuremlops-next.png' alt='AzureML Ops next steps and future work'></figure>"
home-iot,üéõ IoT Panel,Personal Project,Winter 2019,"Merced, California",https://github.com/Mutilar/home-control-panel,"A thought experiment on UI design &amp; tactility",,"<h3>The Idea</h3><p>Smart home control lives behind glass: flat touchscreens and voice assistants with no tactile feedback. Home Control Panel is a physical control surface built from toggle switches, rotary encoders, sliding potentiometers, and 7-segment displays, wall-mounted in a picture frame. A thought experiment on whether physical controls can coexist with modern home automation.</p><figure class='modal-figure'><img src='images/home-iot-wire.png' alt='Home IoT Panel wiring'></figure><h3>How It Works</h3><p>A Raspberry Pi with a 10-inch SunFounder touchscreen serves as the brain, running a web-based UI (<code>index.html</code> + <code>index.css</code>) alongside a Python backend (<code>console.py</code>) and JavaScript controller (<code>console.js</code>) that interface with the physical I/O. The panel has 15 SPST toggle switches with red LED covers, KY-040 rotary encoders with threaded knobs, a double-row sliding linear potentiometer, and a 4-bit 7-segment LED display, all mounted in a Quadro 12√ó20 picture frame backed by a Kydex thermoformable sheet. Total build cost: ~$300.</p><figure class='modal-figure'><img src='images/home-iot-front.png' alt='Home IoT Panel front view'></figure><h3>The Grind</h3><p>Every component was sourced off Amazon and hand-wired. Toggle switches need satisfying throw, encoders need precise detents, sliders need smooth travel; choosing the right parts took as long as wiring them. The panel controls lights, climate, and media from a single wall-mounted surface.</p><figure class='modal-figure'><img src='images/home-iot-touch.png' alt='Home IoT Panel touchscreen'></figure>"
motleymoves,üèÉ MotleyMoves,UCM Final Project,Spring 2019,"Merced, California",https://github.com/plebeiathon/MotleyMoves,"Providing Modesto Moves an all-in-one platform",,"<h3>The Idea</h3><p>Modesto Moves is a nonprofit training club with 250+ participants across Modesto, Ripon, and Manteca, running everything on pen and paper. We built MotleyMoves to digitize the operation: event planning, participant tracking, check-in/check-out, medical records, and race mapping in one mobile-friendly web app.</p><h3>How It Works</h3><p>A serverless C#.NET Azure Function App serves as the API layer, connecting an Azure SQL Database to an HTML/CSS/JS front-end. Google Sign-In handles auth, MapBox renders interactive race course maps with draggable markers for bathrooms, refreshments, and parking, and FullCalendar displays upcoming events. Admins can create events, draw race routes on a map, track attendance with timestamps, and view participant medical info from the browser. We enrolled in Azure Cloud for Nonprofits ($5,000/year in credits) to keep the back-end running at zero cost.</p><h3>The Grind</h3><p>The C# back-end has 15+ Azure Function endpoints handling race point CRUD, GPS position updates, medical record management, and attendance tracking. We redesigned the entire administrative workflow, built it with scalability to neighboring communities in mind, and added an attendance incentive system.</p>"
breeze,üí® Breeze,Keysight IoT Challenge,Fall 2019,"Merced, California",https://github.com/plebeiathon/Breeze,"Promoting air quality awareness is a Breeze",,"<h3>The Idea</h3><p>Breeze turns a smartphone into an IoT air quality sensor by plugging off-the-shelf sensors into the audio aux jack: no Bluetooth pairing, no proprietary hardware, no app downloads. Plug in and start collecting data. A submission to Keysight's IoT Innovation Challenge.</p><figure class='modal-figure'><img src='images/wire-diagram.png' alt='Wire diagram showing analog sensor readout converted to 8kHz PWM for aux jack transmission'></figure><h3>How It Works</h3><p>Sensor data streams through the aux jack into a web dashboard built on Paper Dashboard, where Mapbox GL JS renders air quality heatmaps with interpolated color ramps (blue for clean, red for hazardous). Users can zoom from a global view down to street level, with the heatmap transitioning to individual data point circles at higher zoom. A Stats page shows historical trends via Chartist.js and a My Device page manages connected sensors.</p><figure class='modal-figure'><img src='images/network-diagram.png' alt='Network diagram showing data flow from air quality sensors through microprocessors and phones to database and web application'></figure><h3>The Grind</h3><p>The aux jack approach avoids wireless protocol configuration, companion app installs, and battery-powered sensor nodes. Any phone with a headphone jack becomes a node in a distributed air quality network. The visualization layer is built from GeoJSON data sources with custom heatmap weight, intensity, radius, and opacity interpolations tuned per zoom level.</p>"
firmi,üî¨ Firmi,MACES NASA MUREP,Spring 2018,"Merced, California",https://github.com/Mutilar/Firmi-1,"Teaching abstract physics through tangible 3D-printed models",,"<h3>The Idea</h3><p>Fermi surfaces define the electronic behavior of metals but are hard to visualize from a textbook. As part of the MACES NASA MUREP experience at UC Merced, we used the Firmi utility to turn Fermi surface data into 3D-printed models for in-classroom teaching.</p><h3>How It Works</h3><p>Firmi converts calculated Fermi surfaces into 3D-printable geometry. Starting from electronic structure calculations (Quantum ESPRESSO, Wannier90), the pipeline reads <code>bxsf</code> files in XCrySDen format and applies a Marching Cubes algorithm (implemented in Fortran90) to generate isosurface meshes. The output is an OpenSCAD <code>.scad</code> file that renders the Fermi surface as a solid volume, exported to STL for printing. Examples include fcc copper and fcc lead on 20√ó20√ó20 k-grids.</p><h3>The Grind</h3><p>The Marching Cubes implementation handles non-orthogonal parallelipiped reciprocal lattice vectors with periodic boundary conditions and cell translation to close surfaces. Tuning k-grid resolution is a balancing act: too fine and OpenSCAD runs out of memory, too coarse and the physics is lost. The printed models let students rotate a copper Fermi surface in their hands instead of reading about it in a diagram.</p><figure class='modal-figure'><img src='images/firmi_poster.png' alt='Firmi project poster'><figcaption>Firmi project poster</figcaption></figure>"
ozone,üó† Ozone,Innovate to Grow,Spring 2018,"Merced, California",https://github.com/SSites/Ozone,"Accessing sustainability initiatives interactively","Second Place","<h3>The Idea</h3><p>UC Merced's Department of Sustainability had no central way to show students, faculty, and visitors what green initiatives existed on campus or where to find them. Through Engineering Service Learning, we built Ozone: an interactive web app mapping every sustainability resource on campus, designed to live at sustainability.ucmerced.edu/map.</p><figure class='modal-figure'><img src='images/ozone-desktop.png' alt='Ozone desktop view'></figure><h3>How It Works</h3><p>A React front-end with three themed Mapbox GL JS maps (Environmental, Economic, and Social), each rendering GeoJSON polygon overlays of campus buildings. Point-of-interest markers are loaded from CSV data via Axios and PapaParse, covering water refill stations, Ozzi reusable container stations, LEED labs, EV charging, bike racks, community gardens, bus stops, gender-neutral bathrooms, and more. Each marker gets a unique Font Awesome icon and color, with popups showing descriptions and locations. The maps support geolocation tracking, navigation controls, and a 3D pitch toggle for building extrusions.</p><figure class='modal-figure'><img src='images/ozone-phone.png' alt='Ozone mobile view'></figure><h3>The Grind</h3><p>Every building on UC Merced's campus is hand-mapped as a GeoJSON polygon with precise coordinates: COB1, COB2, SE1, SE2, Mariposa, Tuolumne, Cathedral, Half Dome, Valley Terrace. Three separate map components each load their own themed CSV datasets and render dozens of categorized markers with custom icon switching. Built on Node/Express with Jest testing.</p><figure class='modal-figure'><img src='images/ozone_poster.png' alt='Ozone project poster'></figure>"
iterate,‚Ñπ Iterate,Mobile App Challenge,Winter 2016,"Merced, California",https://github.com/Mutilar/iterate,"The Rosetta Stone of Programming","$5,000 Grand Prize","<h3>The Idea</h3><p>Iterate is a mobile code editor that uses Java/Arduino syntax but lets you build programs through guided tap-based selections instead of raw typing. It sits between block-based tools like Scratch and writing real code. Built for UC Merced's CITRIS Mobile App Challenge.</p><figure class='modal-figure'><img src='images/iterate_judge.png' alt='Iterate judging at CITRIS Mobile App Challenge'></figure><h3>How It Works</h3><p>Built in Unity with C#, the app runs on a 1,300+ line CodeManager that acts as both editor and interpreter. Users tap a line to enter edit mode, then construct code through cascading option menus: Create Variable ‚Üí Integer/Double/Boolean/String ‚Üí name it ‚Üí assign a value. Flow control (if/else, while, for loops), Arduino hardware calls (pinMode, digitalWrite, analogRead, analogWrite, Serial.begin/print/println, servo.attach/write, delay, map), and variable scoping are all handled through the same tap-driven UI. A ColorCoder class provides real-time syntax highlighting (keywords in blue, objects in teal, strings in amber, comments in green) by injecting Unity rich text color tags on every frame. Pre-built lesson templates (Windmill, Servos, Stopwatch) load from Resources as starting scaffolds.</p><figure class='modal-figure'><img src='images/iterate_people.png' alt='Iterate team'></figure><h3>The Grind</h3><p>The editor tracks variable scope by scanning every line above the cursor for declarations, dynamically populating only valid options at each step; you cannot write a syntax error. The whole thing compiles to WebGL for in-browser play on any device. $5,000 Grand Prize at the 2017 CITRIS Mobile App Challenge. <a href='https://iterateco.de' target='_blank'>Try it live ‚Üí</a></p><figure class='modal-figure'><img src='images/iterate_poster.png' alt='Iterate project poster'></figure>"
dogpark,üêï DogPark,Pitchfest,Spring 2017,"Merced, California",,"Tinder for pet adoption",Finalist,"<h3>The Idea</h3><p>Millions of shelter animals are euthanized every year because the adoption process is inconvenient, not because they're unadoptable. DogPark applies the Tinder swipe model to pet adoption: browse shelter animal profiles, match with ones that fit your lifestyle, and connect with the shelter directly.</p><h3>How It Works</h3><p>A mobile-first swipe interface surfaces shelter animal profiles with photos, temperament info, and compatibility details. Users swipe right on animals they're interested in, building a shortlist. The app connects shelters with potential adopters who don't know where to start, reducing the friction between wanting a pet and actually going to a shelter.</p><h3>The Grind</h3><p>Pitched at UC Merced's Pitchfest 2016, selected as a Finalist. Most of the work was in framing the problem and the pitch itself rather than building out a full product.</p>"
amaxesd,‚ö° AMAX ESD,"FIRST Robotics, AMAX",Winter 2014,"Fremont, California",,"ESD for AMAX's ISO 9001 manufacturing",,"<h3>The Idea</h3><p>Electrostatic discharge damages server components in ways that don't show up until well short of expected MTBF. As an ISO 9001 manufacturer building servers for Fortune 500 brands, AMAX needed to detect ESD bracelet disconnections instantly on a busy production floor. FIRST Robotics Team 1458 (the Red Ties) took the project.</p><blockquote><p>In school, we learn a lot of theoretical principles. Our participation in FRC has allowed us to put those theories into practice, but in a game environment. This project with AMAX has taken that one step further and allowed us to understand these principles in a real-world situation.</p><cite>David Hungerman (brother), FRC 1458</cite></blockquote><h3>How It Works</h3><p>Standard ESD bracelets wired through analog ESD monitors with connectivity to LED light poles. When an operator's bracelet loses ground (disconnected or improperly seated) the light pole fires immediately, giving floor-wide visual identification. No software lag, no manual checks: automated monitoring that integrates into AMAX's existing assembly lines.</p><blockquote><p>We are very proud of the members of Team 1458 for accomplishing this project with AMAX. The mission of FIRST is to inspire young people to be science and technology leaders by building skills just like this. The fact that these students were able to put those skills into practical use so early in their careers is a testament to the success of FIRST. This just underscores that FIRST is about more than robots.</p><cite>Jim Beck, California Senior Regional Director, US FIRST</cite></blockquote><h3>The Grind</h3><p>High school students solving a real manufacturing problem for a real company. We went from classroom principles to a product AMAX deployed on their production lines.</p><figure class='modal-figure'><img src='images/amax-pole.png' alt='AMAX ESD light pole alert system on the production floor'></figure>"
