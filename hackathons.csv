ID,NAME,TITLE,DATE,LOCATION,MOTTO,GITHUB,WIN,TEXT
motorskills,ü¶æ MotorSkills,SLO Hacks,February 2019,"San Luis Obispo, California",ML-driven Intelligence for Industrial IoT,https://github.com/plebeiathon/motorskills,Best Use of GCP,"<h3>The Idea</h3><p>Inspired by many a destroyed DC motor ‚Äî one broken motor on an assembly line can cost thousands in downtime and repairs. MotorSkills brings ML-powered intelligence to industrial motors: automated diagnostics, at-a-glance malfunction corrections, and push notifications before things go catastrophic. A distributed, scaleable monitoring solution for factories small and large.</p><h3>How It Works</h3><p>We skim amperage and RPM data from motors and feed it into Google AutoML ‚Äî but the clever trick is converting time-series sensor readings into miniature images, then running them through an image classifier to detect behavioral anomalies like stalls and load spikes. Results hit a front-end UI for visualization and trigger push notifications for critical issues. The back-end runs on Node.JS with serverless function calls and parallel clustering to handle Bluetooth sensor comms alongside AutoML.</p><h3>The Grind</h3><p>We generated our own ML training data by physically stalling, overloading, and abusing DC motors with custom hardware ‚Äî a hands-on crash course in the feedback loop between training data and model accuracy. Wiring together nearly the entire GCP suite into one cohesive hardware app earned us Best Use of GCP and a plan to drag eighteenth-century motor technology into the ML-powered era.</p>"
gasleek,‚õΩ GasLeek,ValleyHacks,January 2019,"Modesto, California",Applying linear regression machine learning to gas prices to help truckers save money at the gas pump,https://github.com/plebeiathon/gasLEEK,First Place,"<h3>The Idea</h3><p>3.5 million truckers refuel 130+ times a year ‚Äî even a few cents per gallon adds up to billions. GasLeek uses machine learning to forecast gas price trends so drivers can time their fill-ups and save real money. The name says it all: gas LinEar rEgression Kode.</p><h3>How It Works</h3><p>We train an LSTM neural network on historical weekly gas price data, converting time-series readings into supervised learning problems with differencing and MinMax scaling. The model outputs persistence forecasts and projected savings, visualized on a React dashboard (Chart.js) with weekly, monthly, and annual price views, a MapBox-powered gas station map, and a volatility-based savings calculator showing the potential impact across the entire US trucking fleet.</p><h3>The Grind</h3><p>Building the full ML pipeline ‚Äî from raw CSV parsing in Python to a Keras LSTM model to a live React front-end pulling data via Axios ‚Äî meant stitching together two completely separate stacks into one cohesive product. We wrangled TensorFlow setup, DeckGL map layers, and real-time CSV parsing, and walked out with a First Place win.</p>"
chemistry,üß™ ChemisTRY,CruzHacks,January 2019,"Santa Cruz, California",Augmented Reality Project Based Learning for Middle-School Chemistry,https://github.com/plebeiathon/ChemisTRY,,"<h3>The Idea</h3><p>Physical project-based learning kits are expensive and single-use ‚Äî most schools can't afford them. We built a digital alternative: AR lesson plans that teach the same standards-aligned chemistry curriculum at a fraction of the cost, with infinite reusability. Students interact with Amazon Alexa in a 3D chemistry sandbox, exploring exothermic reactions and radioactivity right from their classroom.</p><h3>How It Works</h3><p>AR.JS handles the augmented reality layer on a Node.JS back-end wired into a full AWS stack ‚Äî the AWS SDK, Lambda for serverless compute, DynamoDB for NoSQL storage, and the Alexa Developer Console for voice interaction. Point a classroom webcam at a marker, talk to Alexa, and watch chemistry come alive in 3D.</p><h3>The Grind</h3><p>Maintaining a secure, private connection between Alexa and a classroom webcam through AWS was the big beast ‚Äî stitching together that many AWS services into one cohesive app is no small feat. But we came out with a working proof of concept that swaps stale YouTube videos for tangible AR spaces, and we can't wait to see middle schoolers get tricked into learning.</p>"
sriracha,ü¶ø SRIRACHA,SDHacks,October 2018,"San Diego, California",Search & Rescue Informatic Robot Assistant Clearing Hazardous Areas,https://github.com/plebeiathon/sriracha,Third Place,"<h3>The Idea</h3><p>After disasters like Indonesia's devastating earthquake, first responders risk their lives searching collapsed buildings blind. SRIRACHA ‚Äî Search and Rescue Informatic Robot Assisting in Charting Hazardous Areas ‚Äî is a modified RC tank with an oscillating range-finder that rolls in first, constructing 3D point clouds of structures in real time so rescuers know exactly what they're walking into.</p><h3>How It Works</h3><p>An Arduino Mega drives the tank and its sweeping range-finder, streaming sensor data over Bluetooth (HC-06) to a Node.JS server via a custom serial communication protocol. JavaScript parses the incoming data and Pixi.JS renders it into a live 3D point cloud visualization ‚Äî a real-time map of the building as the robot explores it.</p><h3>The Grind</h3><p>Serial communication nearly broke us ‚Äî we burned through multiple libraries before landing on SerialPort for low-level Bluetooth access and real-time data streaming. Designing our own communication protocol from scratch taught us networking fundamentals the hard way. SD Hacks mentors were clutch for debugging Node.JS file streaming and wireless strategy, and we walked out with a Third Place finish and plans to add A* pathfinding for smarter exploration.</p>"
smartank,üöú SMARTank,HackFresno,April 2018,"Fresno, California","An autonomous soil and moisture sensing robot, reducing the barrier of entry to IOT farming",https://github.com/plebeiathon/SMARTank,Best Hardware Hack,"<h3>The Idea</h3><p>Coming from UC Merced ‚Äî deep in California's agricultural heartland ‚Äî we wanted to make IoT farming accessible. Mapping soil moisture across a field normally requires thousands of sensors. SMARTank replaces them with one autonomous robot that roves the land and probes as it goes.</p><h3>How It Works</h3><p>A retrofitted RC tank running on Arduino with upgraded control systems. It methodically traverses farmland, probing soil at regular intervals to record moisture levels and build a data map that would otherwise take a massive sensor grid to produce.</p><h3>The Grind</h3><p>Power management was the core battle ‚Äî our components demanded more amperage than our supply could deliver, forcing constant trade-offs between capability and what the hardware could actually sustain. We pushed through it to take home Best Hardware Hack.</p>"
blindsight,üë®‚Äçü¶Ø Blindsight,CitrusHack,April 2018,"Riverside, California","Giving the visually impaired a new form of haptic sight",https://github.com/plebeiathon/blindsight,Third Place,"<h3>The Idea</h3><p>The walking cane hasn't been upgraded in centuries ‚Äî we fixed that. Two teams with the same vision merged forces to build a wearable haptic system that lets the visually impaired feel their surroundings. An ultrasonic sensor on a stepper motor scans the environment and translates distances into vibrations through an array of haptic modules, all mounted on an infamous top hat.</p><h3>How It Works</h3><p>Built on a Raspberry Pi with 3D-printed parts and a vibration motor array. The ultrasonic sensor sweeps on a stepper motor, mapping distances from the body and converting them into haptic feedback in real time. We deliberately traded raw sensor precision for responsiveness ‚Äî the result was a system reactive enough to detect someone stepping into your path.</p><h3>The Grind</h3><p>Hardware and software chaos in equal measure: a Qualcomm board that refused to connect to WiFi, vague wiring bugs that ate hours, and constant cost-benefit trade-offs between precision and speed. But the moment we navigated a hallway eyes-closed using nothing but vibrations, it all clicked.</p>"
seerauber,üß≠ SeeR√§uber,SacHacks,December 2018,"Sacramento, California","Polygonal Pirates Prowling the Pacific: a distributed-AI pirate game",https://github.com/plebeiathon/seerauber,Second Place,"<h3>The Idea</h3><p>In SeeR√§uber, logical mastery is only the first chapter of the story. You write code to build ships, direct fleets, and govern an empire ‚Äî but fate's hand, revealed through tarot, challenges your carefully laid plans. Each card draw brings new chaos, forcing you to adapt or embrace the unpredictable. Only those who balance logic and fortune will leave a lasting legacy.</p><h3>How It Works</h3><p>Built in Unity3D with C#, the game features a custom visual programming language ‚Äî drag-and-drop code blocks with loops, conditionals, tasks, and variables that control your pirate crew's AI. Each pirate has simulated needs (hunger, thirst, sleep, sailing) and an interpreter evaluates your nested code blocks to generate task queues in real time. A state machine tracks day/night cycles, combat, and sailing conditions, while procedurally generated pirate names and Shakespearean insults bring the crew to life.</p><h3>The Grind</h3><p>We built an entire code interpreter from scratch in 24 hours ‚Äî a recursive block evaluator that parses nested conditionals, loops, and boolean expressions to drive distributed pirate AI. Wiring up the drag-and-drop UI, the pirate need system, and the state machine into a cohesive game loop was a sprint, but we sailed out of SacHacks with a Second Place finish and a playable WebGL build.</p>"
gist,üåæ GISt,HackDavis,January 2018,"Davis, California",Getting the GISt of our food: using AR to fill in the missing link from farm to table,https://github.com/plebeiathon/GISt,Best Environment Hack,"<h3>The Idea</h3><p>Those berries taste great ‚Äî but where are they actually from? Nobody knows, and that means nobody knows the environmental impact either. GISt bridges the gap from farm to table: point your phone at produce and get an interactive AR experience showing where your food comes from, how it got to you, and what that means for the planet.</p><h3>How It Works</h3><p>AR powered by Vuforia through Unity3D, with UX prototyped in Axure RP 8 and built in C#. Farm data, GIS layers, and maps are pulled live via the OSIsoft API, MapBox, JavaScript, and JSON ‚Äî all stitched together into a colorful, intuitive overlay on real-world products.</p><h3>The Grind</h3><p>Android SDK and Java JDK errors nearly killed the mobile build, but we pushed through to a working prototype that nailed the core mechanics. Juggling three separate APIs across AR, mapping, and farm data stretched us across computer science and cognitive science ‚Äî and we walked out with a Best Environment Hack win and a vision for real farm-to-table transparency.</p>"
digestquest,ü•´ DigestQuest,HackMerced,September 2017,"Merced, California","FitBit for your Stomach: data has never been healthier",https://github.com/plebeiathon/DigestQuest,Best in Design,"<h3>The Idea</h3><p>Everyone's pocket holds an image processing powerhouse ‚Äî so we pointed it at nutrition labels. Snap a photo, and DigestQuest instantly breaks down your diet calorie by calorie. No manual entry, no guesswork ‚Äî just pull, snap, and know exactly what you're eating.</p><h3>How It Works</h3><p>We built a web app around Tesseract's JavaScript OCR API, but raw OCR output is messy. So we wrote a custom heuristic algorithm that applies representativeness matching to map garbled text to actual nutritional values. Those parsed results feed into a back-end database and get visualized in an intuitive, at-a-glance display designed for any user.</p><h3>The Grind</h3><p>36 hours of DNS battles, wrangling JQuery and Node.js into talking to Tesseract on a web platform, and obsessing over UX until it felt right. We walked in barely knowing JavaScript or image processing APIs and walked out with a working multi-language pipeline, a one-of-a-kind data viz, and a Best in Design win.</p>"
